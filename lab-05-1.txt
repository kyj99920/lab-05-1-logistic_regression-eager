import numpy as np
#앞으로 셀에서는 약자np. 으로 사용할것임
import matplotlib.pyplot as plt
%matplotlib inline
import tensorflow as tf
#X 와 Y 의 상관관계를 분석하는 기초적인 선형 회귀 모델을 만들고 실행 
import tensorflow.contrib.eager as tfe
#tfe 모듈은 즉시 실행 모드와 그래프 실행 모드 양쪽다 동작하는 다양한 기능들을 포함

tf.enable_eager_execution()
#즉시 실행 (eager execution)을 시작
tf.set_random_seed(777)  # for reproducibility
#재현성을 위해 난수에 부여되는 시드를 777로 설정한 것 
print(tf.__version__)
#출력
x_train = [[1., 2.],
          [2., 3.],
          [3., 1.],
          [4., 3.],
          [5., 3.],
          [6., 2.]]
#x_train 정의
y_train = [[0.],
          [0.],
          [0.],
          [1.],
          [1.],
          [1.]]
#y_train 정의

x_test = [[5.,2.]]
y_test = [[1.]]
#x_test와 y_test 입력


x1 = [x[0] for x in x_train]
x2 = [x[1] for x in x_train]
#x1과 x2 입력


colors = [int(y[0] % 3) for y in y_train]
#색 지정
plt.scatter(x1,x2, c=colors , marker='^')
plt.scatter(x_test[0][0],x_test[0][1], c="red")
#스캐터플롯의 색 지정 및 변경

plt.xlabel("x1")
# 주어진 데이터 x1을 점으로 표시
plt.ylabel("x2")
# 주어진 데이터 x2를 점으로 표시
plt.show()
#명령은 시각화 명령을 실제로 차트로 렌더링(rendering)하고 마우스 움직임 등의 이벤트를 기다리라는 지시

dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))
# training dataset 준비
W = tf.Variable(tf.zeros([2,1]), name='weight')
b = tf.Variable(tf.zeros([1]), name='bias')
#정규분포로부터의 난수값을 반환 

def logistic_regression(features):
    hypothesis  = tf.div(1., 1. + tf.exp(tf.matmul(features, W) + b))
    #hypothesis 가 sigmoid 함수 사용
    return hypothesis
    
def loss_fn(hypothesis, features, labels):
    cost = -tf.reduce_mean(labels * tf.log(logistic_regression(features)) + (1 - labels) * tf.log(1 - hypothesis))
    #cost 함수 정의
    #reduce_mean은 평균을 만들어주는 부분
    return cost

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
#GradientDescent 방법을 통해 최적값 a를 찾아내어서 cost를 최소화하는 방향으로 train하는 것

def accuracy_fn(hypothesis, labels):
    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
    #cast 함수로 hypothesis > 0.5 명제를 floa32 로 변환 
    #hypothesis 가 0.5 보다 크다면 1.을 0.5 보다 작거나 같으면 0.을 return 
    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))
    #equal 함수로 예상값(predicted)과 실제값(Y)이 같은지 비교후
    #True/False 결과를 float32 로 cast 시키고 
    #시행 전체의 평균을 reduce_mean 으로 구함 
    return accuracy

def grad(hypothesis, features, labels):
    with tf.GradientTape() as tape:
    #tape에 계산 과정을 기록해두었다가 tape.gradient를 이용해서 미분을 자동으로 구할 수 있음
        loss_value = loss_fn(logistic_regression(features),features,labels)
        #loss_value 설정
    return tape.gradient(loss_value, [W,b])
    

EPOCHS = 1001
#전체 sample 데이터를 이용하여 1001바퀴 돌며 학습하는 것

for step in range(EPOCHS):
#EPOCHS 반복
    for features, labels  in tfe.Iterator(dataset):
        grads = grad(logistic_regression(features), features, labels)
        #grads 정의
        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))
        if step % 100 == 0:
            print("Iter: {}, Loss: {:.4f}".format(step, loss_fn(logistic_regression(features),features,labels)))
            #step % 100 이 0이면 다음 출력
test_acc = accuracy_fn(logistic_regression(x_test),y_test)
#test_acc 정의
print("Testset Accuracy: {:.4f}".format(test_acc))
#출력
