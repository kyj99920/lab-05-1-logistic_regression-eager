import numpy as np
#앞으로 셀에서는 약자np. 으로 사용할것임
import matplotlib.pyplot as plt
%matplotlib inline
import tensorflow as tf
#X 와 Y 의 상관관계를 분석하는 기초적인 선형 회귀 모델을 만들고 실행 
import tensorflow.contrib.eager as tfe

tf.enable_eager_execution()
#ensorflow 함수들의 동작을 바꾸는 것
tf.set_random_seed(777)  # for reproducibility
#재현성을 위해 난수에 부여되는 시드를 777로 설정한 것 
print(tf.__version__)
#출력
x_train = [[1., 2.],
          [2., 3.],
          [3., 1.],
          [4., 3.],
          [5., 3.],
          [6., 2.]]
y_train = [[0.],
          [0.],
          [0.],
          [1.],
          [1.],
          [1.]]

x_test = [[5.,2.]]
y_test = [[1.]]


x1 = [x[0] for x in x_train]
x2 = [x[1] for x in x_train]

colors = [int(y[0] % 3) for y in y_train]
plt.scatter(x1,x2, c=colors , marker='^')
plt.scatter(x_test[0][0],x_test[0][1], c="red")

plt.xlabel("x1")
plt.ylabel("x2")
# 주어진 데이터들을 점으로 표시
plt.show()
#명령은 시각화 명령을 실제로 차트로 렌더링(rendering)하고 마우스 움직임 등의 이벤트를 기다리라는 지시

dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))
#training dataset 준비
W = tf.Variable(tf.zeros([2,1]), name='weight')
b = tf.Variable(tf.zeros([1]), name='bias')
#정규분포로부터의 난수값을 반환 

def logistic_regression(features):
    hypothesis  = tf.div(1., 1. + tf.exp(tf.matmul(features, W) + b))
    return hypothesis

def loss_fn(hypothesis, features, labels):
    cost = -tf.reduce_mean(labels * tf.log(logistic_regression(features)) + (1 - labels) * tf.log(1 - hypothesis))
    return cost

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
#GradientDescent 방법을 통해 최적값 a를 찾아내어서 cost를 최소화하는 방향으로 train하는 것

def accuracy_fn(hypothesis, labels):
    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
    #cast 함수로 hypothesis > 0.5 명제를 floa32 로 변환 
    #hypothesis 가 0.5 보다 크다면 1.을 0.5 보다 작거나 같으면 0.을 return 
    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))
    #equal 함수로 예상값(predicted)과 실제값(Y)이 같은지 비교후
    #True/False 결과를 float32 로 cast 시키고 #시행 전체의 평균을 reduce_mean 으로 구함 
    return accuracy

def grad(hypothesis, features, labels):
    with tf.GradientTape() as tape:
        loss_value = loss_fn(logistic_regression(features),features,labels)
    return tape.gradient(loss_value, [W,b])

EPOCHS = 1001

for step in range(EPOCHS):
    for features, labels  in tfe.Iterator(dataset):
        grads = grad(logistic_regression(features), features, labels)
        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))
        if step % 100 == 0:
            print("Iter: {}, Loss: {:.4f}".format(step, loss_fn(logistic_regression(features),features,labels)))
test_acc = accuracy_fn(logistic_regression(x_test),y_test)
print("Testset Accuracy: {:.4f}".format(test_acc))
#출력
